{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression in Spark\n",
    "\n",
    "Sushant N. More\n",
    "\n",
    "Ref: Udemy course -- Spark and Python for Big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/sushant/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('logReg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format('libsvm').load('./Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Logistic_Regression/sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegModel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLogRegModel = logRegModel.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegModelSummary = fittedLogRegModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[19.8534775947478...|[0.99999999761359...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.377398194908...|[1.41321555111056...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.401459284891...|[1.25804865126979...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-18.862741612668...|[6.42710509170303...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.483011833009...|[1.27157209200604...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[19.8506078990277...|[0.99999999760673...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.337256674833...|[1.47109814695581...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.595579753418...|[3.08850168102631...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[19.2708803215613...|[0.99999999572670...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.6202328360422...|[0.99999999994480...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logRegModelSummary.predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logRegModelSummary.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = fitModel.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[124,125,126...|[31.0332571034844...|[0.99999999999996...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[27.0510219452602...|[0.99999999999821...|       0.0|\n",
      "|  0.0|(692,[129,130,131...|[15.7084490667890...|[0.99999984937148...|       0.0|\n",
      "|  0.0|(692,[150,151,152...|[21.4585381998360...|[0.99999999952062...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[14.4682882682260...|[0.99999947940257...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[12.6199421180775...|[0.99999669457474...|       0.0|\n",
      "|  0.0|(692,[234,235,237...|[6.46910976510908...|[0.99845179513320...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-18.236290579108...|[1.20248488445532...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-8.5636914948915...|[1.90876795096115...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-23.758430320991...|[4.80667560085456...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionAndLabels.predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEval = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric BinaryClassificationEvaluator() ouputs is roc. The MulticlassClassificationEvaluator can output other things such as F1 score, weighted precission, weighted recall, accuracy etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEval.evaluate(predictionAndLabels.predictions)\n",
    "# perfect classification: not realistic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicData = spark.read.csv('./Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Logistic_Regression/titanic.csv', \n",
    "                            inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanicData.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanicData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanicData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicSelectCols = titanicData.select(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
    " 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------+------------------+------------------+-------------------+-----------------+--------+\n",
      "|summary|           Survived|            Pclass|   Sex|               Age|             SibSp|              Parch|             Fare|Embarked|\n",
      "+-------+-------------------+------------------+------+------------------+------------------+-------------------+-----------------+--------+\n",
      "|  count|                891|               891|   891|               714|               891|                891|              891|     889|\n",
      "|   mean| 0.3838383838383838| 2.308641975308642|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824| 32.2042079685746|    null|\n",
      "| stddev|0.48659245426485753|0.8360712409770491|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|49.69342859718089|    null|\n",
      "|    min|                  0|                 1|female|              0.42|                 0|                  0|              0.0|       C|\n",
      "|    max|                  1|                 3|  male|              80.0|                 8|                  6|         512.3292|       S|\n",
      "+-------+-------------------+------------------+------+------------------+------------------+-------------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanicSelectCols.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's drop the null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicSelectnonNull = titanicSelectCols.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderIndexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After StringIndexer, we gotta One Hot Encode.\n",
    "\n",
    "Say, we have three index: A, B, C.\n",
    "\n",
    "Say using StringIndexer: A -> 0, B -> 1, C -> 2. \n",
    "\n",
    "One hot encoder converts each example into arrays of 0 and 1.\n",
    "\n",
    "For instance if our example is 1, then one hot encoder makes it: [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderEncoder = OneHotEncoder(inputCol = 'SexIndex', outputCol = 'SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarkIndexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkIndex')\n",
    "embarkEncoder = StringIndexer(inputCol='EmbarkIndex', outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['SexVec', 'EmbarkVec', 'Pclass', 'Age', 'SibSp',\n",
    "                                       'Parch', 'Fare'], \n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [genderIndexer, embarkIndexer, genderEncoder, embarkEncoder, assembler, logReg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = titanicSelectnonNull.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fitModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+----+-----+-----+-----+--------+--------+-----------+-------------+---------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|Pclass| Sex| Age|SibSp|Parch| Fare|Embarked|SexIndex|EmbarkIndex|       SexVec|EmbarkVec|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+------+----+----+-----+-----+-----+--------+--------+-----------+-------------+---------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|     1|male|18.0|    1|    0|108.9|       C|     0.0|        1.0|(1,[0],[1.0])|      1.0|[1.0,1.0,1.0,18.0...|[-0.4189622457255...|[0.39676510226684...|       1.0|\n",
      "|       0|     1|male|19.0|    1|    0| 53.1|       S|     0.0|        0.0|(1,[0],[1.0])|      0.0|[1.0,0.0,1.0,19.0...|[-0.3199373081984...|[0.42069102639967...|       1.0|\n",
      "|       0|     1|male|19.0|    3|    2|263.0|       S|     0.0|        0.0|(1,[0],[1.0])|      0.0|[1.0,0.0,1.0,19.0...|[0.08223339231483...|[0.52054677071175...|       0.0|\n",
      "+--------+------+----+----+-----+-----+-----+--------+--------+-----------+-------------+---------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "myEval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7937074829931973"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myEval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    \n",
    "Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyData = spark.read.csv('./Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Logistic_Regression/customer_churn.csv', \n",
    "                             inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|        Onboard_date|            Location|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:...|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:...|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:...|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:...|13120 Daniel Moun...|           Smith Inc|    1|\n",
      "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:...|765 Tricia Row Ka...|          Love-Jones|    1|\n",
      "|Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:...|6187 Olson Mounta...|        Kelly-Warren|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companyData.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date=datetime.datetime(2013, 8, 30, 7, 0, 40), Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1),\n",
       " Row(Names='Kevin Mueller', Age=41.0, Total_Purchase=11916.22, Account_Manager=0, Years=6.5, Num_Sites=11.0, Onboard_date=datetime.datetime(2013, 8, 13, 0, 38, 46), Location='6157 Frank Gardens Suite 019 Carloshaven, RI 17756', Company='Wilson PLC', Churn=1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companyData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companyData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companyData.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count is 900 across all the columns. So looks like we don't have any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Company|count|\n",
      "+--------------------+-----+\n",
      "|      Richardson LLC|    1|\n",
      "|Freeman, Lam and ...|    1|\n",
      "|Hall, Hernandez a...|    1|\n",
      "|          Cannon Inc|    1|\n",
      "|       Smith-Carroll|    1|\n",
      "|Miller, Johnson a...|    1|\n",
      "|          Obrien PLC|    1|\n",
      "|Hunter, Reyes and...|    1|\n",
      "|Smith, Marshall a...|    1|\n",
      "|           Smith PLC|    1|\n",
      "|          Hall Group|    1|\n",
      "|            Todd LLC|    1|\n",
      "|           Lopez PLC|    1|\n",
      "|     Turner and Sons|    1|\n",
      "|     Armstrong-Hicks|    1|\n",
      "|Fisher, Moore and...|    1|\n",
      "|    Simmons and Sons|    1|\n",
      "|Cochran, Molina a...|    1|\n",
      "|       Jimenez Group|    1|\n",
      "|Jones, Black and ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companyData.groupby('Company').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companyData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Company)|\n",
      "+-----------------------+\n",
      "|                    873|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companyData.select(countDistinct('Company')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many companies, it's probably not the best idea to one hot encode it. Let's convert a feature vector using the numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Age', 'Total_Purchase', 'Years', 'Num_Sites'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembledData = assembler.transform(companyData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|        Onboard_date|            Location|             Company|Churn|            features|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:...|10265 Elizabeth M...|          Harvey LLC|    1|[42.0,11066.8,7.2...|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:...|6157 Frank Garden...|          Wilson PLC|    1|[41.0,11916.22,6....|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:...|1331 Keith Court ...|Miller, Johnson a...|    1|[38.0,12884.75,6....|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembledData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToUse = assembledData.select('features', 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([42.0, 11066.8, 7.22, 8.0]), Churn=1),\n",
       " Row(features=DenseVector([41.0, 11916.22, 6.5, 11.0]), Churn=1),\n",
       " Row(features=DenseVector([38.0, 12884.75, 6.67, 12.0]), Churn=1)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataToUse.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataToUse.randomSplit([0.7, 0.3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
